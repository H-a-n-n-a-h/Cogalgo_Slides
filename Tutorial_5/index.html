<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>reveal.js</title>

		<link rel="stylesheet" href="../dist/reset.css">
		<link rel="stylesheet" href="../dist/reveal.css">
		<link rel="stylesheet" href="../dist/theme/black.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="../plugin/highlight/monokai.css">
		<!-- MathJax for LaTeX -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
	</head>

  <style>
:root {
  --nicecyan:   #00CC99;  /* (0,0.8,0.6) */
  --niceblue:   #4D66FF;  /* (0.3,0.4,1) */
  --nicepurple: #FF1AFF;  /* (1,0.1,1) */
  --nicered:    #CC001A;  /* (0.8,0,0.1) */
  --niceyellow: #FFB300;  /* (1,0.7,0) */
  --niceorange: #FF8000;  /* (1,0.5,0) */
  --offwhite:   #FFE6FF;  /* (1,0.9,1) */
}
</style>

	<body>
<div class="reveal">
  <div class="slides">

    <!-- Slide 1: Title -->
    <section>
      <h2 style="color:#A8E6CF;">Cognitive Algorithms: Dimensionality Reduction</h2>
      <p style="color:#EAEAEA;">Hannah Louisa Boldt h.boldt@campus.tu-berlin.de</p>
    </section>

<section>
  <h3 style="color:#A8E6CF; text-align:center; text-transform: none">Organization</h3>
  
    
	<p style="color:#8A8A8A; font-size:0.7em; margin-top:5px;">
Next tutorial will be in person in Mar.0001 again!  
</section>


<section>
  <h3 style="color:#A8E6CF; text-align:center; text-transform: none">Course Structure</h3>
  <svg id="tree1" width="700" height="600"></svg>

  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    const data = {
      name: "Machine Learning",
      children: [
        {
          name: "Supervised Learning",
          children: [
            { name: "Regression", children: [{ name: "Linear Regression", children: [{ name: "Kernel Ridge Regression"}]}] },
            { name: "Classification", children: [{ name: "Binary Classification", children: [{ name: "LDA"},{ name: "Perceptron"},{ name: "NCC"}]}] },
          ]
        },
        {
          name: "Unsupervised Learning",
          children: [
            { name: "Clustering", children: [{ name: "k-Means"}] },
            { name: "Dimension Reduction", children: [{ name: "NMF"}, { name: "PCA", children: [{ name: "kPCA"}]}] }
          ]
        }
      ]
    };

    function drawTree(svgId, highlight=false) {
      const width = 700, height = 600;
      const treeLayout = d3.tree().size([width, height - 100]);
      const root = d3.hierarchy(data);
      treeLayout(root);

      const svg = d3.select(svgId)
        .append("g")
        .attr("transform", "translate(50,50)");

      const highlightNames = new Set([
        "Machine Learning",
        "Unsupervised Learning",
        "Clustering",
        "k-Means",
        "Dimension Reduction",
        "NMF",
        "PCA",
        "kPCA",
      ]);

      function isHighlighted(link) {
        const src = link.source.data.name;
        const tgt = link.target.data.name;
        return highlight && highlightNames.has(src) && highlightNames.has(tgt);
      }

      // Draw links
      svg.selectAll("path")
        .data(root.links())
        .enter()
        .append("path")
        .attr("d", d3.linkVertical().x(d => d.x).y(d => d.y))
        .attr("stroke", d => isHighlighted(d) ? "#00FF9C" : "#A8E6CF")
        .attr("stroke-width", d => isHighlighted(d) ? 3 : 2)
        .attr("fill", "none");

      // Draw nodes
      svg.selectAll("circle")
        .data(root.descendants())
        .enter()
        .append("circle")
        .attr("cx", d => d.x)
        .attr("cy", d => d.y)
        .attr("r", 6)
        .attr("fill", d => (highlight && highlightNames.has(d.data.name)) ? "#00FF9C" : "#FFD166");

      // Draw labels
      svg.selectAll("text")
        .data(root.descendants())
        .enter()
        .append("text")
        .attr("x", d => d.children ? d.x - 10 : d.x)
        .attr("y", d => d.children ? d.y + 5 : d.y + 20)
        .attr("text-anchor", d => d.children ? "end" : "middle")
        .attr("fill", d => (highlight && highlightNames.has(d.data.name)) ? "#00FF9C" : "#EAEAEA")
        .style("font-size", "16px")
        .text(d => d.data.name);
    }

    drawTree("#tree1", false);
  </script>
</section>

<section>
  <h3 style="color:#A8E6CF; text-align:left; text-transform: none">Today</h3>
  <svg id="tree2" width="700" height="600"></svg>
                <!-- Question highlighted -->
  
  <script>
    drawTree("#tree2", true);
  </script>
</section>

<section style="padding:0; margin:0; width:100vw; overflow-x:hidden;">
  <h3 style="color:#A8E6CF; text-align:left; text-transform: none;font-size:0.7em; margin:0;">
    Recap: Supervised Learning
  </h3>

  <table style="width:100%; border-collapse: collapse; color:#EAEAEA; font-size:0.4em; margin:0; table-layout:fixed;">
    <colgroup>
      <col style="width:50%;">
      <col style="width:40%;">
      <col style="width:30%;">
    </colgroup>
    <tr>
      <th style="text-align:center; border-bottom: 2px solid #A8E6CF;">Classification</th>
      <th style="text-align:center; border-bottom: 2px solid #A8E6CF;">Regression</th>
    </tr>
    <tr>
      <td style="padding:5px; vertical-align:top; border-right:1px solid #444; word-wrap: break-word;">
        <p>
          Let $w_{bin}, x_{new} \in \mathbb{R}^d$, $\beta \in \mathbb{R}$.  
          Prediction by:
          \[
            \hat{f}: \mathbb{R}^d \to \{0,1\}
          \]
                \[
            x_{new} \mapsto \mathrm{sign}(w_{bin}^\top x_{new} - \beta) = \hat{y}_{new}
          \]
        </p>
        <p style="color:#8A8A8A; font-size:0.7em; margin-top:5px;">
          $w_{bin}$ and $\beta$ determine the decision boundary
        </p>

        <div style="text-align:center; margin-top:10px;">
          <img 
            src="../img/ncc_perceptron_lda.png" 
            alt="Perceptron Learning Animation"
            style="max-width:100%; height:auto; display:block; margin:0 auto;"
          >
          <p style="color:#8A8A8A; font-size:0.7em; margin-top:5px;">
            Comparison of NCC, LDA, and Perceptron boundaries
          </p>
           <table style="width:90%; margin:20px auto; border-collapse: collapse; color:#EAEAEA; font-size:0.6em;">
    <thead>
      <tr style="background-color:#444;">
        <th style="padding:10px; border:1px solid #666;"></th>
        <th style="padding:10px; border:1px solid #666;">NCC (Nearest Centroid)</th>
        <th style="padding:10px; border:1px solid #666;">LDA (Linear Discriminant Analysis)</th>
        <th style="padding:10px; border:1px solid #666;">Perceptron</th>
      </tr>
    </thead>
    <tbody>
      <tr style="background-color:#333;">
        <td style="padding:10px; border:1px solid #666;">Parameter Calculation</td>
        <td style="padding:10px; border:1px solid #666; text-align:left;">
          \( \mathbf{w} = \mu_0 - \mu_1 \)
          \( \beta = \frac{1}{2} (\mu_0^\top\mu_0 - \mu_1 ^\top \mu_1) \)
        </td>
        <td style="padding:10px; border:1px solid #666; text-align:left;">
          \(\mathbf{w} = \Sigma^{-1} (\mu_1 - \mu_0)\)
          
          \(\beta  = \frac{1}{2}\mathbf{w}^\top \left(\mu_1 + \mu_0 \right)
	+ \log\left( \frac{n_0}{n_1} \right). \)covariance matrix \(\Sigma\)
        </td>
        <td style="padding:10px; border:1px solid #666; text-align:left;">
          Iterative update: \(\mathbf{w}_{aug} \leftarrow \mathbf{w}_{aug} + \eta (y_i - \hat{y}_i) x_i\)
        </td>
      </tr>
      <tr style="background-color:#444;">
        <td style="padding:10px; border:1px solid #666;">Error Function / Loss</td>
        <td style="padding:10px; border:1px solid #666; text-align:left;">
          Minimize distance to centroids \(\hat{y} = \arg\min_c \|x - \mu_c\|^2\)
        </td>
        <td style="padding:10px; border:1px solid #666; text-align:left;">
          Maximizes Fisher criterion
        </td>
        <td style="padding:10px; border:1px solid #666; text-align:left;">
          Perceptron loss: \(\sum_i \max(0, -y_i \mathbf{w}^\top x_i)\)
        </td>
      </tr>
    </tbody>
  </table>
        </div>
      </td>
      <td style="padding:5px; vertical-align:top; word-wrap: break-word;">
        <p>
          Let $w_{lin} \in \mathbb{R}^p$, $x_{new} \in \mathbb{R}^d$, and $\varphi: \mathbb{R}^d \to \mathbb{R}^p$.  
          Prediction by:
          \[
            \hat{f}: \mathbb{R}^d \to \mathbb{R}
          \]
          \[
            x_{new} \mapsto w_{lin}^\top \varphi(x_{new}) = \hat{y}_{new}
          
          \text{ or }
        
         \sum_{i=1}^n \alpha_i k(x_i, x_{new})
        \]
        </p>
        <p style="color:#8A8A8A; font-size:0.7em; margin-top:5px;">
          $w_{lin}$ and $\varphi (bzw. k)$ determine how the predicted function looks
        </p>
         <img 
            src="../img/polyfit_2_on_polynomial.png" 
            alt="Perceptron Learning Animation"
            style="max-width:40%; height:auto; display:block; margin:0 auto;"
          >
         <p style="color:#8A8A8A; font-size:0.7em; margin-top:5px;">
            RR and KRR result the same with same lambda. Computation and prediction comparison:
          </p>
           <table style="width:90%; margin:20px auto; border-collapse: collapse; color:#EAEAEA; font-size:0.6em;">
    <thead>
      <tr style="background-color:#444;">
        <th style="padding:10px; border:1px solid #666;"></th>
        <th style="padding:10px; border:1px solid #666;">Ridge Regression (RR)</th>
        <th style="padding:10px; border:1px solid #666;">Kernel Ridge Regression (KRR)</th>
      </tr>
    </thead>
    <tbody>
      <tr style="background-color:#333;">
        <td style="padding:10px; border:1px solid #666;">Parameter Calculation</td>
        <td style="padding:10px; border:1px solid #666; text-align:left;">
          \(\mathbf{w} = (\varphi(X)^\top \varphi(X) + \lambda I)^{-1} \varphi(X)^\top \mathbf{y}\)
        </td>
        <td style="padding:10px; border:1px solid #666; text-align:left;">
          \(\mathbf{\alpha} = (K + \lambda I)^{-1} \mathbf{y}\)
        </td>
      </tr>
      <tr style="background-color:#444;">
        <td style="padding:10px; border:1px solid #666;">Prediction</td>
        <td style="padding:10px; border:1px solid #666; text-align:left;">
          \( f(\mathbf{x}_{new}) =  \mathbf{w}^\top \varphi(\mathbf{x}_{new}) \)
        </td>
        <td style="padding:10px; border:1px solid #666; text-align:left;">
          \( f(\mathbf{x}_{new}) =  \sum_{i=1}^n \alpha_i k(x_i, x_{new}) \)
        </td>
      </tr>
      <tr style="background-color:#333;">
        <td style="padding:10px; border:1px solid #666;">Error Function</td>
        <td style="padding:10px; border:1px solid #666; text-align:left;">
                  \( \underbrace{ \|\mathbf{y} - \mathbf{w}^\top \varphi(X)\|^2 }_{\text{OLS}}
        + \underbrace{\lambda \|\mathbf{w}\|^2}_{\text{regularization}} \)
        </td>
        <td style="padding:10px; border:1px solid #666; text-align:left;">
          \( \underbrace{ \sum_{i=1}^N \Big(y_i - \sum_{j=1}^N \alpha_j \underbrace{\varphi(\mathbf{x}_j)^\top \varphi(\mathbf{x}_i}_{k(\mathbf{x}_i,\mathbf{x}_j)}) \Big)^2 }_{\text{OLS Dual form}}
        + \underbrace{\lambda \|\mathbf{w}\|^2}_{\text{regularization}} \)
        </td>
      </tr>
    </tbody>
  </table>
      </td>
    </tr>
  </table>
</section>

<section>
  <h3 style="color:#A8E6CF; text-align:left; text-transform: none; font-size:1.1em;">Example: Mapping function from $\mathbb{R}^2$ to $\mathbb{R}^3$</h3>

  <div style="display:flex; justify-content:center; align-items:center; gap:20px; margin-top:10px;">
    <img src="../img/non_linear_2D.png" width="500">
    <img src="../img/non_linear_2D_mapped.gif" width="700">
  </div>

  <p style="color:#8A8A8A; font-size:0.5em; text-align:center; margin-top:5px;">
    Here $\mathcal{X} = \mathbb{R}^2, \mathcal{F} = \mathbb{R}^3,$datapoints $x \in \mathcal{X}$ and the mapping function $\varphi(x) = [ x_1 \quad x_2 \quad  x_1^2 + x_2^2]^\top \in \mathcal{F}$
  </p>
</section>

<section>
  <h3 style="color:#A8E6CF; text-align:middle; text-transform: none; font-size:1.1em;">Dimensionality Reduction</h3>
  <div style="text-align:center;">
     
    <img src="../img/projection_onto_xy.gif" width="500" style="display:block; margin: 0 auto;">
    
      <p style="color:#8A8A8A; font-size:0.5em; text-align:center; margin-top:5px;">
    \[

      \underbrace{\begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \end{bmatrix} }_{\text{projection}}
      X
      = X_{projected} 
    \]
  </p>
  </div>
</section>

<section>
  <h3 style="color:#A8E6CF; text-align:center; text-transform: none">Approach 1: Maximize Variance in Projected Space</h3>
  
  <div style="text-align:center;">
     
    <img src="../img/test.png" width="700" style="display:block; margin: 0 auto;">
    
	   <!-- Question highlighted -->
    <p class="fragment" style="color:#FFD166; font-weight:bold; margin-top:20px;font-size:0.7em;">
      
    </p>
  </div>
</section>

<section>
  <h3 style="color:#A8E6CF; text-align:left; text-transform: none; font-size:1.0em;">Approach 1: Maximize Variance in Projected Space</h3>

  <div style="display:flex; justify-content:center; align-items:center; gap:20px; margin-top:10px;">
    <img src="../img/projection_loop_one_class.gif" width="600">
    <img src="../img/projection_loop_one_class_max_variance.webp" width="600">
  </div>

  <p style="color:#8A8A8A; font-size:0.5em; text-align:center; margin-top:5px;">
    Left shows all possible projections in lower dimensional subspace. Right shows projection one subspace that maximizes variance.
  </p>
  	   <!-- Question highlighted -->
    <p class="fragment" style="color:#FFD166; font-weight:bold; margin-top:20px;font-size:0.7em;">
      Which algorithm from the lecture uses this approach?
    </p>
    <!-- Question highlighted -->
    <p class="fragment" style="color:#FFD166; font-weight:bold; margin-top:20px;font-size:0.7em;">
      PCA!
    </p>
</section>

<section>
  <h3 style="color:#A8E6CF; text-align:left; text-transform: none; font-size:1.0em;">
    Principal Component Analysis
  </h3>

  <div style="margin-top:10px;font-size:0.8em; padding-left:10px;">
    <ol>
      <li>Compute empirical covariance matrix \(\frac{1}{n} XX^\top = \Sigma_x \in \mathbb{R}^{d \times d}\)</li>
      <li>Compute eigenvalues of \(\Sigma_x\)</li>
      <li>
        For the \(k\) largest eigenvalues \(\lambda_1, \dots, \lambda_k \in \mathbb{R}\), calculate corresponding eigenvectors \(w_1, \dots, w_k \in \mathbb{R}^d\)  
        and form the projection matrix:
        \[
          W = 
          \begin{bmatrix}
            | & | & \dots & | \\
            w_1 & w_2 & \dots & w_k \\
            | & | & \dots & |
          \end{bmatrix}
        \]
      </li>
      <li>Project data: \(H = W^\top X\)</li>
      <li>For reconstruction: \(X \approx W \cdot H\)</li>
    </ol>
  </div>

  <p class="fragment" style="color:#FFD166; font-weight:bold; margin-top:20px; font-size:0.7em;">
    For centered data!
  </p>
</section>

<section>
  <h3 style="color:#A8E6CF; text-align:left; text-transform: none; font-size:1.0em;">
    (Linear) kernel PCA
  </h3>

  <div style="margin-top:10px; font-size:0.8em;  padding-left:10px;">
    <ol>
      <li>Compute kernel matrix \(K = X^\top X \in \mathbb{R}^{n \times n}\)</li>
      <li>Compute eigenvalues of \(K\).</li>
      <li>
        For the \(k\) largest eigenvalues \(\lambda_1, \dots, \lambda_k \in \mathbb{R}\), compute corresponding eigenvectors \(v_1, \dots, v_k \in \mathbb{R}^n\).  
        Form matrix \(\alpha\) and projection:
        \[
          \alpha = 
          \begin{bmatrix}
            | & | & \dots & | \\
            v_1 & v_2 & \dots & v_k \\
            | & | & \dots & |
          \end{bmatrix}, \quad
          W = X \alpha
        \]
      </li>
      <li>Project data: \(H = W^\top X\)</li>
      <li>For reconstruction: \(X \approx W \cdot H\)</li>
    </ol>
  </div>

  <p class="fragment" style="color:#FFD166; font-weight:bold; margin-top:20px; font-size:0.7em;">
    For centered data! Same result as PCA, different complexity.
  </p>
</section>



<section>
  <h3 style="color:#A8E6CF; text-align:center; font-size:1.0; text-transform: none">PCA: Plotting with latent variables</h3>
  
  <div style="text-align:center;">
     
    <img src="../img/PCA_visualization_example.png" width="700" style="display:block; margin: 0 auto;">
      <p style="color:#8A8A8A; font-size:0.3em; text-align:center; margin-top:5px;">
    Source: Opportunistic or Non-Random Wildlife Crime? Attractiveness Rather Than Abundance in the Wild Leads to Selective Parrot Poaching
  </p>
  </div>
</section>

<section>
  <h3 style="color:#A8E6CF; text-align:left; text-transform: none; font-size:1.0em;">Approach 2: Clustering</h3>

  <div style="display:flex; justify-content:center; align-items:center; gap:20px; margin-top:10px;">
    <img src="../img/projection_loop_one_class.gif" width="600">
    <img src="../img/projection_loop_one_class_max_variance.webp" width="600">
  </div>

  <p style="color:#8A8A8A; font-size:0.5em; text-align:center; margin-top:5px;">
    Left shows all possible projections in lower dimensional subspace. Right shows projection one subspace that maximizes variance.
  </p>
  	   <!-- Question highlighted -->
    <p class="fragment" style="color:#FFD166; font-weight:bold; margin-top:20px;font-size:0.7em;">
      Which algorithm from the lecture uses this approach?
    </p>
    <!-- Question highlighted -->
    <p class="fragment" style="color:#FFD166; font-weight:bold; margin-top:20px;font-size:0.7em;">
      PCA!
    </p>
</section>

<section>
  <h3 style="color:#A8E6CF; text-align:center; text-transform: none">Definition: Gram Matrix</h3>
  <p style="color:#EAEAEA; text-align:left; font-size:0.8em;">

$\mathbf{x}_1,\mathbf{x}_1$ datapoints, $k$ kernel function
$$
K := 
\begin{bmatrix}
    k(\mathbf{x}_1,\mathbf{x}_1) & k(\mathbf{x}_1,\mathbf{x}_2) & \dots & k(\mathbf{x}_1,\mathbf{x}_n) \\
    k(\mathbf{x}_2,\mathbf{x}_1) & k(\mathbf{x}_2,\mathbf{x}_2) & \dots & k(\mathbf{x}_2,\mathbf{x}_n) \\
    \vdots & \vdots & \ddots & \vdots \\
    k(\mathbf{x}_n,\mathbf{x}_1) & k(\mathbf{x}_n,\mathbf{x}_2) & \dots & k(\mathbf{x}_n,\mathbf{x}_n)
\end{bmatrix}
= 
k(X_{\text{train}}, X_{\text{train}}) \in \mathbb{R}^{n\times n}
$$

  </p>
    <p style="color:#8A8A8A; font-size:0.7em; margin-top:5px;">
		$K$ is the <b>Gram matrix</b> of $k$.  
In each entry \(K_{i,j}\) we have the value of the kernel function on the \(i\)-th and \(j\)-th data point.  
This matrix is symmetric positive semidefinite.
	  </p>
</section>



<section>
  <h3 style="color:#A8E6CF; text-align:center;text-transform: none">Kernel Trick and Mercer's Theorem</h3>

  <svg width="7900" height="560">

    <!-- scalar product -->
    <foreignObject id="phi" x="80" y="100" width="200" height="60" opacity="0">
      <div xmlns="http://www.w3.org/1999/xhtml"
           style="font-size:28px; color:#EAEAEA;">
        \(\varphi(x_i)^{\top}\varphi(x_j)\)
      </div>
    </foreignObject>
    <animate xlink:href="#phi" attributeName="opacity"
             from="0" to="1" dur="0.8s" begin="0s" fill="freeze" />

    <!-- equals sign -->
    <foreignObject id="eq" x="310" y="100" width="40" height="60" opacity="0">
      <div xmlns="http://www.w3.org/1999/xhtml"
           style="font-size:28px; color:#EAEAEA;">
        \(=\)
      </div>
    </foreignObject>
    <animate xlink:href="#eq" attributeName="opacity"
             from="0" to="1" dur="0.8s" begin="1.0s" fill="freeze" />

    <!-- kernel -->
    <foreignObject id="kxx" x="390" y="100" width="200" height="60" opacity="0">
      <div xmlns="http://www.w3.org/1999/xhtml"
           style="font-size:28px; color:#EAEAEA;">
        \(k(x_i, x_j)\)
      </div>
    </foreignObject>
    <animate xlink:href="#kxx" attributeName="opacity"
             from="0" to="1" dur="0.8s" begin="2.4s" fill="freeze" />

    <!-- kernel trick arrow -->
    <path id="kt-arrow"
          d="M 180 80 C 250 20, 450 20, 520 80"
          stroke="#00FF9C" stroke-width="3"
          fill="none" marker-end="url(#arrowhead)" opacity="0"/>
    <animate xlink:href="#kt-arrow" attributeName="opacity"
             from="0" to="1" dur="0.8s" begin="1.6s" fill="freeze" />

    <text id="kt-text" x="350" y="25" font-size="20"
          fill="#00FF9C" opacity="0">
      Kernel Trick

      if $\bx$ enters only in scalar products, then we can replace each scalar product with $k(\bx_i,\bx_j)$.
    </text>
    <animate xlink:href="#kt-text" attributeName="opacity"
             from="0" to="1" dur="0.8s" begin="1.6s" fill="freeze" />

    <!-- Mercer's theorem arrow -->
    <path id="merc-arrow"
          d="M 520 180 C 450 230, 250 230, 170 180"
          stroke="#FFD166" stroke-width="3"
          fill="none" marker-end="url(#arrowhead)" opacity="0"/>
    <animate xlink:href="#merc-arrow" attributeName="opacity"
             from="0" to="1" dur="0.8s" begin="3.6s" fill="freeze" />

    <text id="merc-text" x="310" y="235" font-size="20"
          fill="#FFD166" opacity="0">
      Mercer's Theorem

      If $k$ is a valid kernel function one can construct a
 feature space and a mapping $\varphi$ such that the scalar product in the mapped space is the same as $k$ 


    </text>
    <animate xlink:href="#merc-text" attributeName="opacity"
             from="0" to="1" dur="0.8s" begin="3.6s" fill="freeze" />

    <!-- arrowhead -->
    <defs>
      <marker id="arrowhead"
              markerWidth="10" markerHeight="7"
              refX="9" refY="3.5" orient="auto">
        <polygon points="0 0, 10 3.5, 0 7" fill="#EAEAEA"/>
      </marker>
    </defs>

  </svg>
</section>

<section>
  <h3 style="color:#A8E6CF; text-align:center; text-transform: none">How do we check if a function is a kernel function?</h3>
  <ul style="color:#EAEAEA; text-align:left; font-size:0.8em;">
    <li>Proof by definition (find $\varphi$)</li>
    <li>Show that it is composed of other valid kernel functions.</li>
    <li>Show that it is symmetric and positive semidefinite (show $K$ is PSD and symmetric).</li>
  </ul>

  <p class="fragment" style="color:#FFD166; font-weight:bold; margin-top:20px;font-size:0.7em;">
    Is the function $k:\mathbb{R}^2 \times \mathbb{R}^2 \to \mathbb{R}, \quad k(x,z) = 1 xz^2+x^2$
    a kernel function?
  </p>
</section>


<section>
  <h3 style="color:#A8E6CF; text-align:center; text-transform: none">How do we check if a function is a kernel function?</h3>
 $x_1, x_2 \in \R^d$ Datapoints
  <ul style="color:#EAEAEA; text-align:left; font-size:0.8em;">

    <li>Linear Kernel:
       \begin{align*}
        k:\R^d \times \R^d \to \R, \quad (x_1,x_2) \mapsto x_1^\top x_2
    \end{align*}
    </li>
    <li>Polynomial Kernel: $p \in \N, c \in \R$
       \begin{align*}
        k:\R^d \times \R^d \to \R, \quad (x_1,x_2) \mapsto (x_1^\top x_2 + c)^p
    \end{align*}
    </li>
    <li>  Gaussian kernel:    $\sigma \in \R$, then
    \begin{align*}
        \text{rbf}:\R^d \times \R^d \to \R, \quad (x_1,x_2) \mapsto e^{-\frac{\norm{x_1 -x_2}^2 }{2 \sigma^2}}
    \end{align*}
    </li>
  </ul>
</section>


<section>
  <h3 style="color:#A8E6CF; text-align:center; text-transform: none">Gaussian kernel</h3>
  
  <div style="text-align:center;">
     
    <img src="../img/RBF_Kernel.jpeg" width="700" style="display:block; margin: 0 auto;">

  </div>
</section>

<section>
  <h3 style="color:#A8E6CF; text-align:center; text-transform: none">Geometric interpretation Gaussian kernel</h3>
  
  <div style="text-align:center;">
Datapoints close to x are more similar than datapoints far away from x.
  </div>
</section>

<section>
 Quick Break
   <p style="color:#8A8A8A; font-size:0.7em; margin-top:5px;">
		Kernel Ridge Regression next
	  </p>
</section>


<section>
    <p style="color:#00FF9C; font-weight:bold;  font-size:0.6em;">
     Kernels and Model Evaluation
    </p>
    
    <div class="global-ring">
  <svg id="tree4" width="400" height="400"></svg>
</div>
<script>
  function drawTree(svgId, highlight = false) {
    const svgElement = document.querySelector(svgId);
    const width  = svgElement.clientWidth  || svgElement.getAttribute("width");
    const height = svgElement.clientHeight || svgElement.getAttribute("height");

    const treeLayout = d3.tree().size([width - 60, height - 120]);
    const root = d3.hierarchy(data);
    treeLayout(root);

    const svg = d3.select(svgId)
      .append("g")
      .attr("transform", "translate(30,30)");

    // links
    svg.selectAll("path")
      .data(root.links())
      .enter()
      .append("path")
      .attr("d", d3.linkVertical().x(d => d.x).y(d => d.y))
      .attr("stroke", "#A8E6CF")
      .attr("stroke-width", 2)
      .attr("fill", "none");

    // nodes
    svg.selectAll("circle")
      .data(root.descendants())
      .enter()
      .append("circle")
      .attr("cx", d => d.x)
      .attr("cy", d => d.y)
      .attr("r", 5)
      .attr("fill", "#FFD166");

    // labels
    svg.selectAll("text")
      .data(root.descendants())
      .enter()
      .append("text")
      .attr("x", d => d.x)
      .attr("y", d => d.children ? d.y - 10 : d.y + 15)
      .attr("text-anchor", "middle")
      .attr("fill", "#EAEAEA")
      .style("font-size", "10px")
      .text(d => d.data.name);
  }

  // correct call
  drawTree("#tree4", true);
</script>
  <p style="color:#8A8A8A; font-size:0.7em; margin-top:5px;">
    Kernels and Model Evaluation can be used everywhere here so I put them outside of this Tree.
  </p>
</section>

<section>
  <p style="color:#8A8A8A; font-size:0.7em; margin-top:5px;">
    Example: Ridge Regression $\Rightarrow$ Kernel Ridge Regression
  </p>
</section>

<section>
    <h3 style="color:#A8E6CF; text-align:left; text-transform: none;font-size:1.2em">Recap: Ridge Regression goal (LSQ + regularization)</h3>

  <table style="width: 100%; border-collapse: collapse; border: none;">
    <tr class="fragment">
      <td style="vertical-align: top; width: 20%; border: none;"><b>$\displaystyle 
        \mathcal{E}_{rr}(\mathbf{w})
        $</b></td>
      <td style="vertical-align: top; text-align: left; border: none;">
        $= \mathcal{E}_{LSQ}(\mathbf{w}) +  \underbrace{\lambda \|\mathbf{w}\|^2}_{\text{regularization}}$
      </td>
    </tr>
    <tr class="fragment">
      <td style="border: none;"></td>
      <td style="text-align: left; border: none;">
        $\displaystyle = \|\mathbf{y} - \mathbf{w}^\top \varphi(X)\|^2  +  \underbrace{\lambda \|\mathbf{w}\|^2}_{\text{regularization}}$
      </td>
    </tr>
    <tr class="fragment">
      <td style="border: none;"></td>
      <td style="text-align: left; border: none;">
        $\displaystyle = \left(\sum_{i=1}^N (y_i - \mathbf{w}^\top \varphi(\mathbf{x}_i))^2\right) 
        + \underbrace{\lambda (\mathbf{w}^\top \mathbf{w})}_{\text{regularization}}$
      </td>
    </tr>
  </table>
</section>

<section>
    <h3 style="color:#A8E6CF; text-align:left; text-transform: none;font-size:0.8em">Kernelizing Ridge Regression</h3>
      <p style="color:#FFFFFF; font-size:0.7em; margin-top:5px;text-align:left;">
We can rewrite our existing error function as:
  <table style="width: 100%; border-collapse: collapse; border: none;font-size:0.5em">
    <tr class="fragment">
      <td style="vertical-align: top; width: 20%; border: none;"><b>$\displaystyle 
        \mathcal{E}_{rr}(\mathbf{w})
        $</b></td>
      <td style="vertical-align: top; text-align: left; border: none;">
        $\displaystyle = \left(\sum_{i=1}^N (y_i - \mathbf{w}^\top \varphi(\mathbf{x}_i))^2\right) 
        + \underbrace{\lambda (\mathbf{w}^\top \mathbf{w})}_{\text{regularization}}$
      </td>
    </tr>
       <tr class="fragment">
      <td style="border: none;"></td>
      <td style="text-align: left; border: none;">
        rewrite $\mathbf{w} = \mathbf{\alpha} \varphi(X)$
      </td>
    </tr>
        <tr class="fragment">
      <td style="border: none;"></td>
      <td style="text-align: left; border: none;">
        $\displaystyle = \dots$
      </td>
    </tr>
    <tr class="fragment">
      <td style="border: none;"></td>
      <td style="text-align: left; border: none;">
        $\displaystyle = \mathbf{y}\mathbf{y}^\top - 2 \mathbf{\alpha} \varphi(X)^\top \varphi(X) \mathbf{y}^\top + \mathbf{\alpha} \varphi(X)^\top \varphi(X) \varphi(X)^\top \varphi(X) \mathbf{\alpha} + \lambda \mathbf{\alpha}^\top \varphi(X)^\top \varphi(X)$
      </td>
    </tr>
  </table>
  </p>
    	   <!-- Question highlighted -->
    <p class="fragment" style="color:#FFD166; font-weight:bold; margin-top:20px;font-size:0.7em;">
      The last line is called the dual representation of $\mathcal{E}_{rr}(\mathbf{w})$.
    </p>
        	   <!-- Question highlighted -->
    <p class="fragment" style="color:#FFD166; font-weight:bold; margin-top:20px;font-size:0.7em;">
      How does that help?
    </p>
       <p class="fragment" style="color:#FFD166; font-weight:bold; margin-top:20px;font-size:0.7em;">
      We dont want to use $\varphi$. Data enters only in scalarproducts we can use the kernel trick and replace scalarproducts with $k$.
    </p>
</section>
<section>
  <h3 style="color:#A8E6CF; text-align:center; text-transform: none;">Ridge vs Kernel Ridge Regression</h3>

  <table style="width:90%; margin:20px auto; border-collapse: collapse; color:#EAEAEA; font-size:0.6em;">
    <thead>
      <tr style="background-color:#444;">
        <th style="padding:10px; border:1px solid #666;"></th>
        <th style="padding:10px; border:1px solid #666;">Ridge Regression (RR)</th>
        <th style="padding:10px; border:1px solid #666;">Kernel Ridge Regression (KRR)</th>
      </tr>
    </thead>
    <tbody>
      <tr style="background-color:#333;">
        <td style="padding:10px; border:1px solid #666;">Parameter Calculation</td>
        <td style="padding:10px; border:1px solid #666; text-align:left;">
          \(\mathbf{w} = (\varphi(X)^\top \varphi(X) + \lambda I)^{-1} \varphi(X)^\top \mathbf{y}\)
        </td>
        <td style="padding:10px; border:1px solid #666; text-align:left;">
          \(\mathbf{\alpha} = (K + \lambda I)^{-1} \mathbf{y}\)
        </td>
      </tr>
      <tr style="background-color:#444;">
        <td style="padding:10px; border:1px solid #666;">Prediction</td>
        <td style="padding:10px; border:1px solid #666; text-align:left;">
          \( f(\mathbf{x}_{new}) =  \mathbf{w}^\top \varphi(\mathbf{x}_{new}) \)
        </td>
        <td style="padding:10px; border:1px solid #666; text-align:left;">
          \( f(\mathbf{x}_{new}) =  \sum_{i=1}^n \alpha_i k(x_i, x_{new}) \)
        </td>
      </tr>
      <tr style="background-color:#333;">
        <td style="padding:10px; border:1px solid #666;">Error Function</td>
        <td style="padding:10px; border:1px solid #666; text-align:left;">
                  \( \|\mathbf{y} - \mathbf{w}^\top \varphi(X)\|^2 
        + \underbrace{\lambda \|\mathbf{w}\|^2}_{\text{regularization}} \)
        </td>
        <td style="padding:10px; border:1px solid #666; text-align:left;">
          \( \|\mathbf{y} - \mathbf{w}^\top \varphi(X)\|^2 
        + \underbrace{\lambda \|\mathbf{w}\|^2}_{\text{regularization}} \)
        </td>
      </tr>
    </tbody>
  </table>
         <p class="fragment" style="color:#FFD166; font-weight:bold; margin-top:20px;font-size:0.7em;">
      RR needs to compute inverse of $\R^p \times \R^p$ matrix and KRR of $\R^n \times \R^n$. What are $p$ and $n$?</section>


<section>
  <h3 style="color:#A8E6CF; text-align:center; text-transform: none">$\sigma$ vs $\lambda$</h3>
 todo
</section>



<section>
  <h3 style="color:#A8E6CF; text-align:center; text-transform: none">Combination overview</h3>

  <div style="display:flex; justify-content:center; align-items:center; gap:20px; margin-top:10px;">
    <img src="../img/grid_train.png" width="600">
    <img src="../img/grid_test.png" width="600">
  </div>

  <p style="color:#8A8A8A; font-size:0.7em; text-align:center; margin-top:5px;">
    Darker "better" (lower mean squared error)
  </p>
</section>


<section>
  <h3 style="color:#A8E6CF; text-align:left; text-transform: none">Standard approach</h3>
    Split data into two sets
      <p style="color:#8A8A8A; font-size:0.8em; margin-top:5px;">
		
      
        \begin{align*}
        X =
        \underbrace{[x_1,x_2,x_3,x_4,x_5,x_6,x_7, }_{\text{train on this}}
        \underbrace{ x_8,x_9]}_{\text{test on this}}
        
        \end{align*}
      </p>
          	   <!-- Question highlighted -->
    <p class="fragment" style="color:#FFD166; font-weight:bold; margin-top:20px;font-size:0.7em;">
    We are not using the full dataset!
    </p>
</section>

<section>
  <h3 style="color:#A8E6CF; text-align:left; text-transform: none">Cross validation</h3>
  
  <p>
    Split data k-times:
    <br>
        <p style="color:#8A8A8A; font-size:0.8em; margin-top:5px;">
		
	  <mathjax>
    
    \begin{align*}

    \text{Fold 2} &= 
      \underbrace{[x_1,x_2}_{\text{test}} , 
      \underbrace{x_3,x_4,x_5,x_6,x_7,x_8,x_9]}_{\text{train }}\\
          \text{Fold 2} &= 
      \underbrace{[x_1,x_2}_{\text{train }} , 
      \underbrace{x_3,x_4}_{\text{test }} , 
      \underbrace{x_5,x_6,x_7,x_8,x_9]}_{\text{train }}\\
        \text{Fold i} &= \dots\\
          \text{Fold k} &= 
      \underbrace{[x_1,x_2,x_3,x_4,x_5,x_6,x_7}_{\text{train }} , 
      \underbrace{x_8,x_9]}_{\text{test }} \\
    \end{align*}
    </mathjax>
    </p>
    Return average test error
  </p>
  
  <!-- Question highlighted -->
  <p class="fragment" style="color:#FFD166; font-weight:bold; margin-top:20px; font-size:0.7em;">
    Why do we rotate the test fold each time? What is the benefit of cross-validation?
  </p>
</section>

<section>
  <h3 style="color:#A8E6CF; text-align:center; text-transform: none">Summary: Linear regression</h3>
  
  <div style="text-align:center;"> 
    <img src="../img/polyfit_2_on_polynomial.png" width="700" style="display:block; margin: 0 auto;">
  </div>
    <p style="color:#8A8A8A; font-size:0.7em; margin-top:5px;">
		Good fit on polynomial function.
	  </p>
    <p class="fragment" style="color:#FFD166; font-weight:bold; margin-top:20px;font-size:0.7em;">
     Linear? Yes linear in weights w.
    </p>
</section>

<section>
  <h3 style="color:#A8E6CF; text-align:center; text-transform: none">Recap: NCC vs. LDA vs. Perceptron</h3>

  <div style="text-align:center; width:100%; height:100%;">
    <img 
      src="../img/ncc_perceptron_lda.png" 
      alt="Perceptron Learning Animation"
      style="
        max-width: 100%; 
        max-height: 80vh; 
        width: auto; 
        height: auto; 
        display: block; 
        margin: 0 auto;
      "
    >

    <!-- Optional caption -->
  <p style="color:#8A8A8A; font-size:0.7em; margin-top:5px;">
	Comparison of NCC, LDA, and Perceptron decision boundaries
	  </p>

  </div>
</section>

    <section>
      <h3 style="color:#A8E6CF; text-align:center; text-transform: none">Questions for upcoming Homework?</h3>
    </section>


		<script src="../dist/reveal.js"></script>
		<script src="../plugin/notes/notes.js"></script>
		<script src="../plugin/markdown/markdown.js"></script>
		<script src="../plugin/highlight/highlight.js"></script>
		<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
		<script src="../plugin/math/math.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,
  				transition: 'none',           // default: no transition
  				backgroundTransition: 'none', // default: no background fade
				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.MathJax3  ],
				  math: {
					  mathjax: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js",
					config: {
					tex: {
						macros: {
						R: "\\mathbb{R}",
						bw: "\\mathbf{w}",
						bx: "\\mathbf{x}",
						sign: "\\operatorname{sign}"
						}
					}
					}
				}
			});
		</script>
	</body>
</html>