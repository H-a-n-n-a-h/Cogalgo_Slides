<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>reveal.js</title>

		<link rel="stylesheet" href="../dist/reset.css">
		<link rel="stylesheet" href="../dist/reveal.css">
		<link rel="stylesheet" href="../dist/theme/black.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="../plugin/highlight/monokai.css">
		<!-- MathJax for LaTeX -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
	</head>

  <style>
:root {
  --nicecyan:   #00CC99;  /* (0,0.8,0.6) */
  --niceblue:   #4D66FF;  /* (0.3,0.4,1) */
  --nicepurple: #FF1AFF;  /* (1,0.1,1) */
  --nicered:    #CC001A;  /* (0.8,0,0.1) */
  --niceyellow: #FFB300;  /* (1,0.7,0) */
  --niceorange: #FF8000;  /* (1,0.5,0) */
  --offwhite:   #FFE6FF;  /* (1,0.9,1) */
}
</style>

	<body>
<div class="reveal">
  <div class="slides">

    <!-- Slide 1: Title -->
    <section>
      <h2 style="color:#A8E6CF;">Cognitive Algorithms: Dimensionality Reduction</h2>
      <p style="color:#EAEAEA;">Hannah Louisa Boldt h.boldt@campus.tu-berlin.de</p>
    </section>

<section>
  <h3 style="color:#A8E6CF; text-align:center; text-transform: none">Organization</h3>
  
    
	<p style="color:#8A8A8A; font-size:0.7em; margin-top:5px;">
Next tutorial will be in person in Mar.0001 again!  
</section>


<section>
  <h3 style="color:#A8E6CF; text-align:center; text-transform: none">Course Structure</h3>
  <svg id="tree1" width="700" height="600"></svg>

  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    const data = {
      name: "Machine Learning",
      children: [
        {
          name: "Supervised Learning",
          children: [
            { name: "Regression", children: [{ name: "Linear Regression", children: [{ name: "Kernel Ridge Regression"}]}] },
            { name: "Classification", children: [{ name: "Binary Classification", children: [{ name: "LDA"},{ name: "Perceptron"},{ name: "NCC"}]}] },
          ]
        },
        {
          name: "Unsupervised Learning",
          children: [
            { name: "Clustering", children: [{ name: "k-Means"}] },
            { name: "Dimension Reduction", children: [{ name: "NMF"}, { name: "PCA", children: [{ name: "kPCA"}]}] }
          ]
        }
      ]
    };

    function drawTree(svgId, highlight=false) {
      const width = 700, height = 600;
      const treeLayout = d3.tree().size([width, height - 100]);
      const root = d3.hierarchy(data);
      treeLayout(root);

      const svg = d3.select(svgId)
        .append("g")
        .attr("transform", "translate(50,50)");

      const highlightNames = new Set([
        "Machine Learning",
        "Unsupervised Learning",
        "Clustering",
        "k-Means",
        "Dimension Reduction",
        "NMF",
        "PCA",
        "kPCA",
      ]);

      function isHighlighted(link) {
        const src = link.source.data.name;
        const tgt = link.target.data.name;
        return highlight && highlightNames.has(src) && highlightNames.has(tgt);
      }

      // Draw links
      svg.selectAll("path")
        .data(root.links())
        .enter()
        .append("path")
        .attr("d", d3.linkVertical().x(d => d.x).y(d => d.y))
        .attr("stroke", d => isHighlighted(d) ? "#00FF9C" : "#A8E6CF")
        .attr("stroke-width", d => isHighlighted(d) ? 3 : 2)
        .attr("fill", "none");

      // Draw nodes
      svg.selectAll("circle")
        .data(root.descendants())
        .enter()
        .append("circle")
        .attr("cx", d => d.x)
        .attr("cy", d => d.y)
        .attr("r", 6)
        .attr("fill", d => (highlight && highlightNames.has(d.data.name)) ? "#00FF9C" : "#FFD166");

      // Draw labels
      svg.selectAll("text")
        .data(root.descendants())
        .enter()
        .append("text")
        .attr("x", d => d.children ? d.x - 10 : d.x)
        .attr("y", d => d.children ? d.y + 5 : d.y + 20)
        .attr("text-anchor", d => d.children ? "end" : "middle")
        .attr("fill", d => (highlight && highlightNames.has(d.data.name)) ? "#00FF9C" : "#EAEAEA")
        .style("font-size", "16px")
        .text(d => d.data.name);
    }

    drawTree("#tree1", false);
  </script>
</section>

<section>
  <h3 style="color:#A8E6CF; text-align:left; text-transform: none">Today</h3>
  <svg id="tree2" width="700" height="600"></svg>
                <!-- Question highlighted -->
  
  <script>
    drawTree("#tree2", true);
  </script>
</section>

<section style="padding:0; margin:0; width:100vw;">
  <h3 style="color:#A8E6CF; text-align:left; text-transform: none;font-size:0.7em; margin:0;">
    Recap: Supervised Learning
  </h3>

  <table style="width:100%; border-collapse: collapse; color:#EAEAEA; font-size:0.4em; margin:0; table-layout:fixed;">
    <colgroup>
      <col style="width:50%;">
      <col style="width:40%;">
      <col style="width:30%;">
    </colgroup>
    <tr>
      <th style="text-align:center; border-bottom: 2px solid #A8E6CF;">Classification</th>
      <th style="text-align:center; border-bottom: 2px solid #A8E6CF;">Regression</th>
    </tr>
    <tr>
      <td style="padding:5px; vertical-align:top; border-right:1px solid #444; word-wrap: break-word;">
          <div style="text-align:center; margin-top:10px;">
          <img 
            src="../img/ncc_perceptron_lda.png" 
            alt="Perceptron Learning Animation"
            style="max-width:100%; height:auto; display:block; margin:0 auto;"
          >
          <p style="color:#8A8A8A; font-size:0.7em; margin-top:5px;">
            Comparison of NCC, LDA, and Perceptron boundaries.  $w_{bin}$ and $\beta$ determine the decision boundary
          </p>
          </div>
          
        <p>
          Let $w_{bin}, x_{new} \in \mathbb{R}^d$, $\beta \in \mathbb{R}$.  
          Prediction by:
          \[
            \hat{f}: \mathbb{R}^d \to \{0,1\}
          \]
                \[
            x_{new} \mapsto \mathrm{sign}(w_{bin}^\top x_{new} - \beta) = \hat{y}_{new}
          \]
        </p>
  

  
           <table style="margin:0px ; border-collapse: collapse; color:#EAEAEA; font-size:0.5em;">
    <thead>
      <tr style="background-color:#444;">
        <th style="padding:10px; border:1px solid #666;"></th>
        <th style="padding:10px; border:1px solid #666;">NCC (Nearest Centroid)</th>
        <th style="padding:10px; border:1px solid #666;">LDA <br>(Linear Discriminant Analysis)</th>
        <th style="padding:10px; border:1px solid #666;">Perceptron</th>
      </tr>
    </thead>
    <tbody>
      <tr style="background-color:#333;">
        <td style="padding:10px; border:1px solid #666;">Parameter Calculation</td>
        <td style="padding:10px; border:1px solid #666; text-align:left;">
          \( \mathbf{w} = \mu_0 - \mu_1 \)
          \( \beta = \frac{1}{2} (\mu_0^\top\mu_0 - \mu_1 ^\top \mu_1) \)
        </td>
        <td style="padding:10px; border:1px solid #666; text-align:left;">
          \(\mathbf{w} = \Sigma^{-1} (\mu_1 - \mu_0)\)
          
          \(\beta  = \frac{1}{2}\mathbf{w}^\top \left(\mu_1 + \mu_0 \right)
	+ \log\left( \frac{n_0}{n_1} \right). \) <br>
  covariance matrix \(\Sigma\)
        </td>
        <td style="padding:10px; border:1px solid #666; text-align:left;">
          Iterative update: \(\mathbf{w}_{aug} \leftarrow \mathbf{w}_{aug} + \eta (y_i - \hat{y}_i) x_i\)
        </td>
      </tr>
      <tr style="background-color:#444;">
        <td style="padding:10px; border:1px solid #666;">Error Function / Loss</td>
        <td style="padding:10px; border:1px solid #666; text-align:left;">
          Minimize distance to centroids \(\hat{y} = \arg\min_c \|x - \mu_c\|^2\)
        </td>
        <td style="padding:10px; border:1px solid #666; text-align:left;">
          Maximizes Fisher criterion
        </td>
        <td style="padding:10px; border:1px solid #666; text-align:left;">
          Perceptron loss: \(\sum_i \max(0, -y_i \mathbf{w}^\top x_i)\)
        </td>
      </tr>
    </tbody>
  </table>
        </div>
      </td>
      <td style="padding:0px; vertical-align:top; word-wrap: break-word;">
          <div style="text-align:center; margin-top:0px;"> 
          <div style="
          display:flex;
          gap:0;
          align-items:stretch;
        ">
          <img
            src="../img/simple_linear_regression.png"
            style="height:120px; width:auto; display:block;"
          >
          <img
            src="../img/polyfit_2_on_polynomial.png"
            style="height:120px; width:auto; display:block;"
          >
            <img
            src="../img/polyfit_50_on_polynomial.png"
            style="height:120px; width:auto; display:block;"
          >

        </div>
         <p style="color:#8A8A8A; font-size:0.7em; margin-top:0px;">
            Linear regression with different basis functions. $w_{lin}$ and $\varphi (bzw. k)$ determine how the predicted function looks
          </p>
          </div> 
        <p>
          Let $w_{lin} \in \mathbb{R}^p$, $x_{new} \in \mathbb{R}^d$, and $\varphi: \mathbb{R}^d \to \mathbb{R}^p$ then 
          \[
            \hat{f}: \mathbb{R}^d \to \mathbb{R}
          \]
          \[
            x_{new} \mapsto w_{lin}^\top \varphi(x_{new}) = \hat{y}_{new}
          
          \text{ or }
        
         \sum_{i=1}^n \alpha_i k(x_i, x_{new})
        \]
        </p>
        <p style="color:#8A8A8A; font-size:0.7em; margin-top:5px;">
      
        </p>
   
           <table style="width:90%; margin:20px auto; border-collapse: collapse; color:#EAEAEA; font-size:0.6em;">
    <thead>
      <tr style="background-color:#444;">
        <th style="padding:10px; border:1px solid #666;"></th>
        <th style="padding:10px; border:1px solid #666;">Ridge Regression (RR)</th>
        <th style="padding:10px; border:1px solid #666;">Kernel Ridge Regression (KRR)</th>
      </tr>
    </thead>
    <tbody>
      <tr style="background-color:#333;">
        <td style="padding:10px; border:1px solid #666;">Parameter Calculation</td>
        <td style="padding:10px; border:1px solid #666; text-align:left;">
          \(\mathbf{w} = (\varphi(X)^\top \varphi(X) + \lambda I)^{-1} \varphi(X)^\top \mathbf{y}\)
        </td>
        <td style="padding:10px; border:1px solid #666; text-align:left;">
          \(\mathbf{\alpha} = (K + \lambda I)^{-1} \mathbf{y}\)
        </td>
      </tr>
      <tr style="background-color:#444;">
        <td style="padding:10px; border:1px solid #666;">Prediction</td>
        <td style="padding:10px; border:1px solid #666; text-align:left;">
          \( f(\mathbf{x}_{new}) =  \mathbf{w}^\top \varphi(\mathbf{x}_{new}) \)
        </td>
        <td style="padding:10px; border:1px solid #666; text-align:left;">
          \( f(\mathbf{x}_{new}) =  \sum_{i=1}^n \alpha_i k(x_i, x_{new}) \)
        </td>
      </tr>
      <tr style="background-color:#333;">
        <td style="padding:10px; border:1px solid #666;">Error Function</td>
        <td style="padding:10px; border:1px solid #666; text-align:left;">
                  \( \underbrace{ \|\mathbf{y} - \mathbf{w}^\top \varphi(X)\|^2 }_{\text{OLS}}
        + \underbrace{\lambda \|\mathbf{w}\|^2}_{\text{regularization}} \)
        </td>
        <td style="padding:10px; border:1px solid #666; text-align:left;">
          \( \underbrace{ \sum_{i=1}^N \Big(y_i - \sum_{j=1}^N \alpha_j \underbrace{\varphi(\mathbf{x}_j)^\top \varphi(\mathbf{x}_i}_{k(\mathbf{x}_i,\mathbf{x}_j)}) \Big)^2 }_{\text{OLS Dual form}}
        + \underbrace{\lambda \|\mathbf{w}\|^2}_{\text{regularization}} \)
        </td>
      </tr>
    </tbody>
  </table>
      </td>
    </tr>
  </table>
</section>

<section>
  <h3 style="color:#A8E6CF; text-align:left; text-transform: none; font-size:1.1em;">Example: Mapping function from $\mathbb{R}^2$ to $\mathbb{R}^3$</h3>

  <div style="display:flex; justify-content:center; align-items:center; gap:20px; margin-top:10px;">
    <img src="../img/non_linear_2D.png" width="500">
    <img src="../img/non_linear_2D_mapped.gif" width="700">
  </div>

  <p style="color:#8A8A8A; font-size:0.5em; text-align:center; margin-top:5px;">
    Here $\mathcal{X} = \mathbb{R}^2, \mathcal{F} = \mathbb{R}^3,$datapoints $x \in \mathcal{X}$ and the mapping function $\varphi(x) = [ x_1 \quad x_2 \quad  x_1^2 + x_2^2]^\top \in \mathcal{F}$
  </p>
</section>

<section>
  <h3 style="color:#A8E6CF; text-align:middle; text-transform: none; font-size:1.1em;">Dimensionality Reduction</h3>
  <div style="text-align:center;">
     
    <img src="../img/projection_onto_xy.gif" width="500" style="display:block; margin: 0 auto;">
    
      <p style="color:#8A8A8A; font-size:0.5em; text-align:center; margin-top:5px;">
    \[

      \underbrace{\begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \end{bmatrix} }_{\text{projection}}
      X
      = X_{projected} 
    \]
  </p>
  </div>
</section>



<section>
  <h3 style="color:#A8E6CF; text-align:left; text-transform: none; font-size:1.0em;">Approach 1: Maximize Variance in Projected Space</h3>

  <div style="display:flex; justify-content:center; align-items:center; gap:20px; margin-top:10px;">
    <img src="../img/projection_loop_one_class.gif" width="600">
    <img src="../img/projection_loop_one_class_max_variance.webp" width="600">
  </div>

  <p style="color:#8A8A8A; font-size:0.5em; text-align:center; margin-top:5px;">
    Left shows all possible projections in lower dimensional subspace. Right shows projection one subspace that maximizes variance.
  </p>

</section>

<section>
  <h3 style="color:#A8E6CF; text-align:center; text-transform: none">Approach 1: Maximize Variance in Projected Space</h3>
  
  <div style="text-align:center;">
     
    <img src="../img/projection_loop_one_class_max_variance.webp" width="700" style="display:block; margin: 0 auto;">
    
  	   <!-- Question highlighted -->
    <p class="fragment" style="color:#FFD166; font-weight:bold; margin-top:20px;font-size:0.7em;">
      Which algorithm from the lecture uses this approach?
    </p>
    <!-- Question highlighted -->
    <p class="fragment" style="color:#FFD166; font-weight:bold; margin-top:20px;font-size:0.7em;">
      PCA!
    </p>
  </div>
</section>

<section>
  <h3 style="color:#A8E6CF; text-align:left; text-transform: none; font-size:1.0em;">
    Principal Component Analysis
  </h3>

  <div style="margin-top:10px;font-size:0.8em; padding-left:10px;">
    <ol>
      <li>Compute empirical covariance matrix \(\frac{1}{n} XX^\top = \Sigma_x \in \mathbb{R}^{d \times d}\)</li>
      <li>Compute eigenvalues of \(\Sigma_x\)</li>
      <li>
        For the \(k\) largest eigenvalues \(\lambda_1, \dots, \lambda_k \in \mathbb{R}\), calculate corresponding eigenvectors \(w_1, \dots, w_k \in \mathbb{R}^d\)  
        and form the projection matrix:
        \[
          W = 
          \begin{bmatrix}
            | & | & \dots & | \\
            w_1 & w_2 & \dots & w_k \\
            | & | & \dots & |
          \end{bmatrix}
        \]
      </li>
      <li>Project data: \(H = W^\top X\)</li>
      <li>For reconstruction: \(X \approx W \cdot H\)</li>
    </ol>
  </div>

  <p class="fragment" style="color:#FFD166; font-weight:bold; margin-top:20px; font-size:0.7em;">
    For centered data!
  </p>
</section>

<section>
  <h3 style="color:#A8E6CF; text-align:left; text-transform: none; font-size:1.0em;">
    (Linear) kernel PCA
  </h3>

  <div style="margin-top:10px; font-size:0.8em;  padding-left:10px;">
    <ol>
      <li>Compute kernel matrix \(K = X^\top X \in \mathbb{R}^{n \times n}\)</li>
      <li>Compute eigenvalues of \(K\).</li>
      <li>
        For the \(k\) largest eigenvalues \(\lambda_1, \dots, \lambda_k \in \mathbb{R}\), compute corresponding eigenvectors \(v_1, \dots, v_k \in \mathbb{R}^n\).  
        Form matrix \(\alpha\) and projection:
        \[
          \alpha = 
          \begin{bmatrix}
            | & | & \dots & | \\
            v_1 & v_2 & \dots & v_k \\
            | & | & \dots & |
          \end{bmatrix}, \quad
          W = X \alpha
        \]
      </li>
      <li>Project data: \(H = W^\top X\)</li>
      <li>For reconstruction: \(X \approx W \cdot H\)</li>
    </ol>
  </div>

  <p class="fragment" style="color:#FFD166; font-weight:bold; margin-top:20px; font-size:0.7em;">
    For centered data! Same result as PCA, different complexity.
  </p>
</section>



<section>
  <h3 style="color:#A8E6CF; text-align:center; font-size:1.0; text-transform: none">PCA: Plotting with latent variables</h3>
  
  <div style="text-align:center;">
     
    <img src="../img/PCA_visualization_example.png" width="700" style="display:block; margin: 0 auto;">
      <p style="color:#8A8A8A; font-size:0.3em; text-align:center; margin-top:5px;">
    Source: Opportunistic or Non-Random Wildlife Crime? Attractiveness Rather Than Abundance in the Wild Leads to Selective Parrot Poaching
  </p>
  </div>
</section>

<section>
  <h3 style="color:#A8E6CF; text-align:left; text-transform: none; font-size:1.0em;">Approach 2: Clustering</h3>

  <div style="display:flex; justify-content:center; align-items:center; gap:20px; margin-top:10px;">
    <img src="../img/test.png" width="400">
    <img src="../img/test.png" width="400">
  </div>

  <p style="color:#8A8A8A; font-size:0.5em; text-align:center; margin-top:5px;">
    Left shows all possible projections in lower dimensional subspace. Right shows projection one subspace that maximizes variance.
  </p>
  	   <!-- Question highlighted -->
    <p class="fragment" style="color:#FFD166; font-weight:bold; margin-top:20px;font-size:0.7em;">
      Which algorithm from the lecture uses this approach?
    </p>
    <!-- Question highlighted -->
    <p class="fragment" style="color:#FFD166; font-weight:bold; margin-top:20px;font-size:0.7em;">
      k Means!
    </p>
</section>

<section>
  <h3 style="color:#A8E6CF; text-align:left; text-transform:none; font-size:1.0em;">
    k-means algorithm
  </h3>

  <div style="margin-top:10px; font-size:0.6em; padding-left:10px;">
    <ol>

        Let \(X \in \mathbb{R}^{n \times d}\) be the data matrix containing
        \(n\) data points \(\mathbf{x}_i \in \mathbb{R}^d\).
    

      <li>
        Initialize \(k\) cluster vectors \(c_j \in \mathbb{R}^d\) randomly then
        \[
          C =
          \begin{bmatrix}
            | & | & \dots & | \\
            c_1 & c_2 & \dots & c_k \\
            | & | & \dots & |
          \end{bmatrix}
          \in \mathbb{R}^{d \times k}.
        \]
        Each column represents one cluster center.
      </li>

      <li>
        
        For each datapoint $\mathbf{x}_i$ assign to clusterpoint based on distance  
        then assignment matrix
        \[
          Y =
          \begin{bmatrix}
            | & | & \dots & | \\
            y_1 & y_2 & \dots & y_k \\
            | & | & \dots & |
          \end{bmatrix}
          \in \{0,1\}^{n \times k},
        \]
        where \(y_{ij} = 1\) iff \(\mathbf{x}_i\) is assigned to cluster \(j\).
      </li>

      <li>
        Update each cluster center by  the mean of all data points
        assigned to that cluster.
      </li>

      <li>
        Repeat 2. and 3. until either a maximum number of iterations is
        reached or the cluster centers change by less than
        \(\varepsilon > 0\).
      </li>

    </ol>
  </div>
</section>


  <p class="fragment" style="color:#FFD166; font-weight:bold; margin-top:20px; font-size:0.7em;">
    For centered data! Same result as PCA, different complexity.
  </p>
</section>





<section>
 Quick Break
   <p style="color:#8A8A8A; font-size:0.7em; margin-top:5px;">
		Kernel Ridge Regression next
	  </p>
</section>


<section>
    <p style="color:#00FF9C; font-weight:bold;  font-size:0.6em;">
     Kernels and Model Evaluation
    </p>
    
    <div class="global-ring">
  <svg id="tree4" width="400" height="400"></svg>
</div>
<script>
  function drawTree(svgId, highlight = false) {
    const svgElement = document.querySelector(svgId);
    const width  = svgElement.clientWidth  || svgElement.getAttribute("width");
    const height = svgElement.clientHeight || svgElement.getAttribute("height");

    const treeLayout = d3.tree().size([width - 60, height - 120]);
    const root = d3.hierarchy(data);
    treeLayout(root);

    const svg = d3.select(svgId)
      .append("g")
      .attr("transform", "translate(30,30)");

    // links
    svg.selectAll("path")
      .data(root.links())
      .enter()
      .append("path")
      .attr("d", d3.linkVertical().x(d => d.x).y(d => d.y))
      .attr("stroke", "#A8E6CF")
      .attr("stroke-width", 2)
      .attr("fill", "none");

    // nodes
    svg.selectAll("circle")
      .data(root.descendants())
      .enter()
      .append("circle")
      .attr("cx", d => d.x)
      .attr("cy", d => d.y)
      .attr("r", 5)
      .attr("fill", "#FFD166");

    // labels
    svg.selectAll("text")
      .data(root.descendants())
      .enter()
      .append("text")
      .attr("x", d => d.x)
      .attr("y", d => d.children ? d.y - 10 : d.y + 15)
      .attr("text-anchor", "middle")
      .attr("fill", "#EAEAEA")
      .style("font-size", "10px")
      .text(d => d.data.name);
  }

  // correct call
  drawTree("#tree4", true);
</script>
  <p style="color:#8A8A8A; font-size:0.7em; margin-top:5px;">
    Kernels and Model Evaluation can be used everywhere here so I put them outside of this Tree.
  </p>
</section>

<section>
  <p style="color:#8A8A8A; font-size:0.7em; margin-top:5px;">
    Example: Ridge Regression $\Rightarrow$ Kernel Ridge Regression
  </p>
</section>

<section>
    <h3 style="color:#A8E6CF; text-align:left; text-transform: none;font-size:1.2em">Recap: Ridge Regression goal (LSQ + regularization)</h3>

  <table style="width: 100%; border-collapse: collapse; border: none;">
    <tr class="fragment">
      <td style="vertical-align: top; width: 20%; border: none;"><b>$\displaystyle 
        \mathcal{E}_{rr}(\mathbf{w})
        $</b></td>
      <td style="vertical-align: top; text-align: left; border: none;">
        $= \mathcal{E}_{LSQ}(\mathbf{w}) +  \underbrace{\lambda \|\mathbf{w}\|^2}_{\text{regularization}}$
      </td>
    </tr>
    <tr class="fragment">
      <td style="border: none;"></td>
      <td style="text-align: left; border: none;">
        $\displaystyle = \|\mathbf{y} - \mathbf{w}^\top \varphi(X)\|^2  +  \underbrace{\lambda \|\mathbf{w}\|^2}_{\text{regularization}}$
      </td>
    </tr>
    <tr class="fragment">
      <td style="border: none;"></td>
      <td style="text-align: left; border: none;">
        $\displaystyle = \left(\sum_{i=1}^N (y_i - \mathbf{w}^\top \varphi(\mathbf{x}_i))^2\right) 
        + \underbrace{\lambda (\mathbf{w}^\top \mathbf{w})}_{\text{regularization}}$
      </td>
    </tr>
  </table>
</section>

<section>
    <h3 style="color:#A8E6CF; text-align:left; text-transform: none;font-size:0.8em">Kernelizing Ridge Regression</h3>
      <p style="color:#FFFFFF; font-size:0.7em; margin-top:5px;text-align:left;">
We can rewrite our existing error function as:
  <table style="width: 100%; border-collapse: collapse; border: none;font-size:0.5em">
    <tr class="fragment">
      <td style="vertical-align: top; width: 20%; border: none;"><b>$\displaystyle 
        \mathcal{E}_{rr}(\mathbf{w})
        $</b></td>
      <td style="vertical-align: top; text-align: left; border: none;">
        $\displaystyle = \left(\sum_{i=1}^N (y_i - \mathbf{w}^\top \varphi(\mathbf{x}_i))^2\right) 
        + \underbrace{\lambda (\mathbf{w}^\top \mathbf{w})}_{\text{regularization}}$
      </td>
    </tr>
       <tr class="fragment">
      <td style="border: none;"></td>
      <td style="text-align: left; border: none;">
        rewrite $\mathbf{w} = \mathbf{\alpha} \varphi(X)$
      </td>
    </tr>
        <tr class="fragment">
      <td style="border: none;"></td>
      <td style="text-align: left; border: none;">
        $\displaystyle = \dots$
      </td>
    </tr>
    <tr class="fragment">
      <td style="border: none;"></td>
      <td style="text-align: left; border: none;">
        $\displaystyle = \mathbf{y}\mathbf{y}^\top - 2 \mathbf{\alpha} \varphi(X)^\top \varphi(X) \mathbf{y}^\top + \mathbf{\alpha} \varphi(X)^\top \varphi(X) \varphi(X)^\top \varphi(X) \mathbf{\alpha} + \lambda \mathbf{\alpha}^\top \varphi(X)^\top \varphi(X)$
      </td>
    </tr>
  </table>
  </p>
    	   <!-- Question highlighted -->
    <p class="fragment" style="color:#FFD166; font-weight:bold; margin-top:20px;font-size:0.7em;">
      The last line is called the dual representation of $\mathcal{E}_{rr}(\mathbf{w})$.
    </p>
        	   <!-- Question highlighted -->
    <p class="fragment" style="color:#FFD166; font-weight:bold; margin-top:20px;font-size:0.7em;">
      How does that help?
    </p>
       <p class="fragment" style="color:#FFD166; font-weight:bold; margin-top:20px;font-size:0.7em;">
      We dont want to use $\varphi$. Data enters only in scalarproducts we can use the kernel trick and replace scalarproducts with $k$.
    </p>
</section>
<section>
  <h3 style="color:#A8E6CF; text-align:center; text-transform: none;">Ridge vs Kernel Ridge Regression</h3>

  <table style="width:90%; margin:20px auto; border-collapse: collapse; color:#EAEAEA; font-size:0.6em;">
    <thead>
      <tr style="background-color:#444;">
        <th style="padding:10px; border:1px solid #666;"></th>
        <th style="padding:10px; border:1px solid #666;">Ridge Regression (RR)</th>
        <th style="padding:10px; border:1px solid #666;">Kernel Ridge Regression (KRR)</th>
      </tr>
    </thead>
    <tbody>
      <tr style="background-color:#333;">
        <td style="padding:10px; border:1px solid #666;">Parameter Calculation</td>
        <td style="padding:10px; border:1px solid #666; text-align:left;">
          \(\mathbf{w} = (\varphi(X)^\top \varphi(X) + \lambda I)^{-1} \varphi(X)^\top \mathbf{y}\)
        </td>
        <td style="padding:10px; border:1px solid #666; text-align:left;">
          \(\mathbf{\alpha} = (K + \lambda I)^{-1} \mathbf{y}\)
        </td>
      </tr>
      <tr style="background-color:#444;">
        <td style="padding:10px; border:1px solid #666;">Prediction</td>
        <td style="padding:10px; border:1px solid #666; text-align:left;">
          \( f(\mathbf{x}_{new}) =  \mathbf{w}^\top \varphi(\mathbf{x}_{new}) \)
        </td>
        <td style="padding:10px; border:1px solid #666; text-align:left;">
          \( f(\mathbf{x}_{new}) =  \sum_{i=1}^n \alpha_i k(x_i, x_{new}) \)
        </td>
      </tr>
      <tr style="background-color:#333;">
        <td style="padding:10px; border:1px solid #666;">Error Function</td>
        <td style="padding:10px; border:1px solid #666; text-align:left;">
                  \( \|\mathbf{y} - \mathbf{w}^\top \varphi(X)\|^2 
        + \underbrace{\lambda \|\mathbf{w}\|^2}_{\text{regularization}} \)
        </td>
        <td style="padding:10px; border:1px solid #666; text-align:left;">
          \( \|\mathbf{y} - \mathbf{w}^\top \varphi(X)\|^2 
        + \underbrace{\lambda \|\mathbf{w}\|^2}_{\text{regularization}} \)
        </td>
      </tr>
    </tbody>
  </table>
         <p class="fragment" style="color:#FFD166; font-weight:bold; margin-top:20px;font-size:0.7em;">
      RR needs to compute inverse of $\R^p \times \R^p$ matrix and KRR of $\R^n \times \R^n$. What are $p$ and $n$?</section>


<section>
  <h3 style="color:#A8E6CF; text-align:center; text-transform: none">$\sigma$ vs $\lambda$</h3>
 todo
</section>



<section>
  <h3 style="color:#A8E6CF; text-align:center; text-transform: none">Combination overview</h3>

  <div style="display:flex; justify-content:center; align-items:center; gap:20px; margin-top:10px;">
    <img src="../img/grid_train.png" width="600">
    <img src="../img/grid_test.png" width="600">
  </div>

  <p style="color:#8A8A8A; font-size:0.7em; text-align:center; margin-top:5px;">
    Darker "better" (lower mean squared error)
  </p>
</section>


<section>
  <h3 style="color:#A8E6CF; text-align:left; text-transform: none">Standard approach</h3>
    Split data into two sets
      <p style="color:#8A8A8A; font-size:0.8em; margin-top:5px;">
		
      
        \begin{align*}
        X =
        \underbrace{[x_1,x_2,x_3,x_4,x_5,x_6,x_7, }_{\text{train on this}}
        \underbrace{ x_8,x_9]}_{\text{test on this}}
        
        \end{align*}
      </p>
          	   <!-- Question highlighted -->
    <p class="fragment" style="color:#FFD166; font-weight:bold; margin-top:20px;font-size:0.7em;">
    We are not using the full dataset!
    </p>
</section>

<section>
  <h3 style="color:#A8E6CF; text-align:left; text-transform: none">Cross validation</h3>
  
  <p>
    Split data k-times:
    <br>
        <p style="color:#8A8A8A; font-size:0.8em; margin-top:5px;">
		
	  <mathjax>
    
    \begin{align*}

    \text{Fold 2} &= 
      \underbrace{[x_1,x_2}_{\text{test}} , 
      \underbrace{x_3,x_4,x_5,x_6,x_7,x_8,x_9]}_{\text{train }}\\
          \text{Fold 2} &= 
      \underbrace{[x_1,x_2}_{\text{train }} , 
      \underbrace{x_3,x_4}_{\text{test }} , 
      \underbrace{x_5,x_6,x_7,x_8,x_9]}_{\text{train }}\\
        \text{Fold i} &= \dots\\
          \text{Fold k} &= 
      \underbrace{[x_1,x_2,x_3,x_4,x_5,x_6,x_7}_{\text{train }} , 
      \underbrace{x_8,x_9]}_{\text{test }} \\
    \end{align*}
    </mathjax>
    </p>
    Return average test error
  </p>
  
  <!-- Question highlighted -->
  <p class="fragment" style="color:#FFD166; font-weight:bold; margin-top:20px; font-size:0.7em;">
    Why do we rotate the test fold each time? What is the benefit of cross-validation?
  </p>
</section>

<section>
  <h3 style="color:#A8E6CF; text-align:center; text-transform: none">Summary: Linear regression</h3>
  
  <div style="text-align:center;"> 
    <img src="../img/polyfit_2_on_polynomial.png" width="700" style="display:block; margin: 0 auto;">
  </div>
    <p style="color:#8A8A8A; font-size:0.7em; margin-top:5px;">
		Good fit on polynomial function.
	  </p>
    <p class="fragment" style="color:#FFD166; font-weight:bold; margin-top:20px;font-size:0.7em;">
     Linear? Yes linear in weights w.
    </p>
</section>

<section>
  <h3 style="color:#A8E6CF; text-align:center; text-transform: none">Recap: NCC vs. LDA vs. Perceptron</h3>

  <div style="text-align:center; width:100%; height:100%;">
    <img 
      src="../img/ncc_perceptron_lda.png" 
      alt="Perceptron Learning Animation"
      style="
        max-width: 100%; 
        max-height: 80vh; 
        width: auto; 
        height: auto; 
        display: block; 
        margin: 0 auto;
      "
    >

    <!-- Optional caption -->
  <p style="color:#8A8A8A; font-size:0.7em; margin-top:5px;">
	Comparison of NCC, LDA, and Perceptron decision boundaries. 
  
	  </p>

  </div>
</section>

    <section>
      <h3 style="color:#A8E6CF; text-align:center; text-transform: none">Questions for upcoming Homework?</h3>
    </section>


		<script src="../dist/reveal.js"></script>
		<script src="../plugin/notes/notes.js"></script>
		<script src="../plugin/markdown/markdown.js"></script>
		<script src="../plugin/highlight/highlight.js"></script>
		<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
		<script src="../plugin/math/math.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,
  				transition: 'none',           // default: no transition
  				backgroundTransition: 'none', // default: no background fade
				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.MathJax3  ],
				  math: {
					  mathjax: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js",
					config: {
					tex: {
						macros: {
						R: "\\mathbb{R}",
						bw: "\\mathbf{w}",
						bx: "\\mathbf{x}",
						sign: "\\operatorname{sign}"
						}
					}
					}
				}
			});
		</script>
	</body>
</html>